- BERT is Bidirectional Encoder Representations from Transformers
- BERT is based on the transformer architecture, but it is only composed of encoder layers
# BERT Architecture
# Input Representation
## 1. Token Embeddings
- Each word in the input sequence is tokenized into WordPiece tokens.
- The token embeddings are taken from a pre-trained vocabulary that BERT uses, where each token is mapped to a fixed-size embedding vector.
## 2. Segment Embeddings
## 3. Position Embeddings
